{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', shuffle_files= False, with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)  ##casts (converts) a variable into a given date type\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "#Normally we'd like to scale our data in some way to make the result more numerically stable (e.g inputs between o to 1)\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255. #used dot(.) to make it float\n",
    "    return image, label\n",
    "\n",
    "#dataset.map(*function*) applies a custom transformation to a given dataset. It takes as input a function which determines the transformation\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# We need to shuffle the data so they are distributed equally in every portion\n",
    "# When we are dealing with enormous datasets, we can't shuffle all the data at once\n",
    "Buffer_size = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(Buffer_size)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "## dataset.batch(batch_size) a mthod that combines the consecutive elements of a dataset into batches.\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "## As we only run the model on the validation set, we only propagate forward. We just calculate the validation loss. On average it should equal to the training loss.\n",
    "\n",
    "#Mnist data is iterable and in 2-tuple format (as_supervised = true)\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100 # The uderlying assumption is that all hidden layers are of the same size\n",
    "\n",
    "## tf.keras.layers.Flatten(original shape) transforms ((flatten)) tensor into a vector.\n",
    "## tf.keras.layers.Dense(output size) takes the inputs, provided to the model and calculate the dot product of the inputs and the weights and adds the bias. This is also where we can apply activation function. \n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation = 'softmax') #since it works with probability\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer,loss) configure the model for training\n",
    "model.compile(optimizer='adam', loss ='sparse_categorical_crossentropy', metrics = ['accuracy']) #sparse_categorical_crossentropy applies one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 3s - loss: 0.3308 - accuracy: 0.9055 - val_loss: 0.1783 - val_accuracy: 0.9495\n",
      "Epoch 2/5\n",
      "540/540 - 2s - loss: 0.1400 - accuracy: 0.9588 - val_loss: 0.1207 - val_accuracy: 0.9685\n",
      "Epoch 3/5\n",
      "540/540 - 2s - loss: 0.0980 - accuracy: 0.9706 - val_loss: 0.0884 - val_accuracy: 0.9745\n",
      "Epoch 4/5\n",
      "540/540 - 2s - loss: 0.0734 - accuracy: 0.9770 - val_loss: 0.0792 - val_accuracy: 0.9787\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0596 - accuracy: 0.9813 - val_loss: 0.0629 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1eb222d9910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "# What happens inside an epoch\n",
    "\n",
    "## At the beginning of eachepoch, the training loss will be set to 0\n",
    "## The algorithm will iterate over a preset number of batches, all from train_data\n",
    "## The weights and biases will be updated as many times as there are batches\n",
    "## We will get a value for the loss function, indicating how training is going\n",
    "## We will also see training accuracy\n",
    "## At the end of each epochs, the algorithm will forward propogate the whole validation set.\n",
    "\n",
    "### Note- When we reach maximum number of epochs the trainning will be over.\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs,validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0820 - accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.65%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_TF2.0]",
   "language": "python",
   "name": "conda-env-py3_TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
